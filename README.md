# ğŸš— Volanteomaleta.com Vehicle Data Scraper

Simple and efficient web scraper to extract vehicle data from volanteomaleta.com.

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
npm install
```

### 2. Configure the Scraper

Edit `scraper.js` and set your input/output files:

```javascript
const CONFIG = {
    platesFile: path.join(__dirname, 'data/plates/plates-1.csv'),
    vehicleDataFile: path.join(__dirname, 'data/results/vehicle-data.csv'),
    progressFile: path.join(__dirname, 'progress/scraping-progress.json'),
}
```

### 3. Run the Scraper

```bash
node scraper.js
```

That's it! The scraper will:
- âœ… Load plates from your CSV
- âœ… Scrape volanteomaleta.com for each plate
- âœ… Save results to `vehicle-data.csv`
- âœ… Track progress automatically
- âœ… Resume if interrupted

## ğŸ“ Project Structure

```
cloudflare-demo/
â”œâ”€â”€ scraper.js                    # Main scraper (all-in-one)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ plates/                   # Input: plate CSV files
â”‚   â”‚   â”œâ”€â”€ plates-1.csv
â”‚   â”‚   â”œâ”€â”€ plates-2.csv
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ results/
â”‚       â””â”€â”€ vehicle-data.csv      # Output: scraped vehicle data
â”œâ”€â”€ progress/
â”‚   â””â”€â”€ scraping-progress.json    # Auto-saved progress
â””â”€â”€ generate-update-sql.js        # SQL generator for database updates
```

## ğŸ“Š Input Format

Your plates CSV should look like:
```csv
Plate Number
ABC123
XYZ789
```

## ğŸ“ Output Format

Results are saved in CSV with these columns:
- Row #
- Plate Number
- Vehicle Type
- Brand
- Model
- Owner RUT
- Engine Number
- Year
- Owner Name
- Scraping Date
- Source Website

## âš™ï¸ Features

- **Auto-Resume**: Stops and starts? No problem! Progress is saved automatically every 10 plates
- **Smart Retries**: Failed requests retry up to 3 times
- **Progress Tracking**: See real-time stats every 100 plates
- **Minimal Browser**: Small, minimized window - no distracting popups
- **Respectful Delays**: 0.5s delay between requests

## ğŸ”„ Resume Scraping

If the scraper stops (Ctrl+C or crash), just run it again:
```bash
node scraper.js
```

It will automatically continue from where it left off using `progress/scraping-progress.json`.

## ğŸ¯ Processing Multiple Plate Files

Want to scrape different files? Just change the config in `scraper.js`:

```javascript
// First run
const CONFIG = {
    platesFile: path.join(__dirname, 'data/plates/plates-1.csv'),
    vehicleDataFile: path.join(__dirname, 'data/results/vehicle-data.csv'),
    progressFile: path.join(__dirname, 'progress/scraping-progress.json'),
}

// Later, for second batch
const CONFIG = {
    platesFile: path.join(__dirname, 'data/plates/plates-2.csv'),
    vehicleDataFile: path.join(__dirname, 'data/results/vehicle-data.csv'),  // Same output file
    progressFile: path.join(__dirname, 'progress/scraping-progress-2.json'), // Different progress file
}
```

## ğŸ› ï¸ Generate SQL Updates

After scraping, generate SQL update statements:

```bash
node generate-update-sql.js
```

This creates `update-leads.sql` with UPDATE statements for your database.

## ğŸ“ˆ Monitor Progress

Check `progress/scraping-progress.json` to see:
- Current index
- Total plates
- Success/error counts
- Percentage complete
- Timestamp

Or watch the console output for real-time stats!

## âš¡ Performance

- **Speed**: ~500ms per plate (7,200 plates/hour)
- **Success Rate**: ~84% (plates with data found)
- **Cost**: Essentially free (just electricity)

## ğŸ›‘ Stop Gracefully

Press `Ctrl+C` to stop. Progress is automatically saved, and you can resume anytime.

## ğŸ“¦ Dependencies

- `puppeteer` - Browser automation
- Node.js built-in modules (fs, path)

## ğŸ“„ License

MIT - See LICENSE file

---

**Note**: Be respectful when scraping. This tool includes delays between requests to avoid overloading the target website.
